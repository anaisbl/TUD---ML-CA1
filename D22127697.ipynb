{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA1 - Machine learning\n",
    "Develop a classifier to predict the outcome of a bank's marketing campaign using the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import, exploration & preprocessing\n",
    "This section will go over exploring and preprocesing the training set of the bank data marketing campaign. The first step will look for missing values, then explore the value counts of some of the input features, as well as the distribution of values for features of integer types. Lastly, I will preprocess the input features and target variable appropriately for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "bank_data = pd.read_csv(\"data/trainingset.txt\")\n",
    "bank_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows and columns\n",
    "print(\"Number of columns: \", len(bank_data.columns))\n",
    "print(\"Number of rows: \", len(bank_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data type of columns\n",
    "print(\"Data types of columns:\")\n",
    "bank_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "bank_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count value unknown across all columns\n",
    "val_to_count = \"unknown\"\n",
    "unknown_counts = {}\n",
    "for col in bank_data.columns:\n",
    "    count = bank_data[col].value_counts().get(val_to_count, 0)\n",
    "    unknown_counts[col] = count \n",
    "\n",
    "print(\"Value counts of '\", val_to_count, \"' across all columns: \")\n",
    "for col, count in unknown_counts.items():\n",
    "    print(col + \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Unknown' is a value present in certain feature columns, and can be interpreted as NaN. I decided to count its occurence across all columns to see if it is statistically significant. It is significant in the 'poutcome' feature, constitutes 80% of total values. It is also significant in 'contact' so we can't remove it. We can however remove 'unknown' values in education and job columns as it constitutes 4.3% & 0.65% of total values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unknown in education & job\n",
    "bank_data.drop(bank_data[bank_data[\"education\"] == \"unknown\"].index, inplace=True)\n",
    "bank_data.drop(bank_data[bank_data[\"job\"] == \"unknown\"].index, inplace=True)\n",
    "\n",
    "# check it has been removed\n",
    "print(\"Value counts: \",bank_data[\"education\"].value_counts(),\n",
    "      \"\\n\\nValue counts: \",bank_data[\"job\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unknown counts remaining in contact and poutcome\n",
    "print(\"Value counts: \",bank_data[\"contact\"].value_counts(),\n",
    "      \"\\n\\nValue counts: \",bank_data[\"poutcome\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'unknown' value count is still significant in the 'contact' and 'poutcome' columns after removing it from the others. Therefore, we leave as is and we proceed with the next step data preprocessing which is encoding.\n",
    "Columns with only two possible categorical values will be binary encoded, the ones with more will be one hot encoded. The target variable will alse be binary encoded.\n",
    "The day & month columns will be combined into a date column, I chose the year 2023 as it has not been specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary encode default, housing and loan\n",
    "cols_to_bencode = [\"default\", \"housing\", \"loan\"]\n",
    "data_bencoded = pd.get_dummies(bank_data, columns=cols_to_bencode, drop_first=True)\n",
    "data_bencoded.columns = data_bencoded.columns.str.replace('yes', 'encoded')\n",
    "\n",
    "# one-hot encode remaining categorical columns\n",
    "data_ohencoded = pd.get_dummies(bank_data, columns=[\"job\", \"marital\", \"education\", \"contact\", \"poutcome\"])\n",
    "\n",
    "# binary encode target variable Y\n",
    "# false is Type A, True is TypeB\n",
    "target_encoded = pd.get_dummies(bank_data, columns=[\"y\"], drop_first=True)\n",
    "target_encoded.columns = target_encoded.columns.str.replace('TypeB', 'encoded')\n",
    "target_encoded.columns = target_encoded.columns.str.replace('TypeA', 'encoded')\n",
    "\n",
    "# combine one-hot encoded & binary encoded dataframes\n",
    "combined_encoded = pd.concat([data_bencoded, data_ohencoded, target_encoded], axis=1)\n",
    "\n",
    "# drop duplicate cols\n",
    "combined_encoded = combined_encoded.loc[:,~combined_encoded.columns.duplicated()]\n",
    "\n",
    "# drop non-encoded cols\n",
    "combined_encoded.drop([\"job\", \"marital\",\"education\",\"contact\",\"poutcome\",\"default\", \"housing\", \"loan\", \"y\"], axis=1, inplace=True)\n",
    "\n",
    "# combine day & month into one column\n",
    "combined_encoded['date'] = combined_encoded['day'].astype(str) + '-' + combined_encoded['month']\n",
    "\n",
    "# convert to datetime type\n",
    "combined_encoded['date'] = pd.to_datetime(combined_encoded['date'], format='%d-%b')\n",
    "\n",
    "# set year as 2023\n",
    "combined_encoded['date'] = combined_encoded['date'] + pd.offsets.DateOffset(years=2023 - combined_encoded['date'].dt.year.max())\n",
    "\n",
    "# remove day & month columns from df\n",
    "combined_encoded.drop(['day', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# check new dataframe\n",
    "combined_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I plotted the values of integer data type to see their distribution and assess whether they needed to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of int type features\n",
    "cols_distribution = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(cols_distribution), ncols=1, figsize=(6, 3 * len(cols_distribution)))\n",
    "\n",
    "# plot each column\n",
    "for i, col in enumerate(cols_distribution):\n",
    "    axes[i].hist(combined_encoded[col], bins=20, color='blue', alpha=0.7)\n",
    "    axes[i].set_xlabel('Values')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that the column \"duration\" only had one type of value = 0, which I confirmed by checking the counts of values. I deduced that the column did not bring any value to predicting the target variable; therefore I removed it. In addition, most of the value distributions skew to the left and have one value towering over the others, therefore I will apply StandardScaler() normalization to even it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value counts of 'duration'\n",
    "combined_encoded[\"duration\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration' column\n",
    "combined_encoded.drop([\"duration\"], axis=1, inplace=True)\n",
    "combined_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe\n",
    "scaled_df = combined_encoded.copy()\n",
    "\n",
    "# normalize age, balance, campaign, pdays & previous\n",
    "cols_scale = [\"age\", \"balance\",\"campaign\",\"pdays\",\"previous\"]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_df = scaler.fit_transform(scaled_df[cols_scale])\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=cols_scale)\n",
    "scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view new distribution of scaled features\n",
    "fig, axes = plt.subplots(nrows=len(cols_scale), ncols=1, figsize=(6, 3 * len(cols_scale)))\n",
    "\n",
    "for i, col in enumerate(cols_scale):\n",
    "    axes[i].hist(scaled_df[col], bins=20, color='green', alpha=0.7)\n",
    "    axes[i].set_xlabel('Values')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of scaled {col}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution remains heavily skewed to the left for 'previous', 'campaign' and 'balance'. They also all have (except for age) one bin significantly larger than the others, especially 'pdays'. This can potentially affect ML models that make assumptions about normal distribution of the data like linear regression. The last step of the preprocessing is to combine scaled features dataframe with the encoded features dataframe to create a final one ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaled_df.add_prefix('scaled_')\n",
    "combined_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# combine scaled & encoded dataframes\n",
    "final_bank = pd.concat([combined_encoded, scaled_df], axis=1)\n",
    "\n",
    "# drop non-encoded cols\n",
    "final_bank.drop([\"age\", \"previous\",\"campaign\",\"balance\",\"pdays\"], axis=1, inplace=True)\n",
    "\n",
    "# convert date column to int64 for model training\n",
    "final_bank['date'] = final_bank['date'].astype(\"int64\")\n",
    "\n",
    "# check all columns are there\n",
    "final_bank.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any missing values as a result of preprocessing\n",
    "final_bank.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns in dataframe\n",
    "final_order = [\"scaled_age\", \"job_JobCat1\",\"job_JobCat2\",\"job_JobCat3\",\"job_JobCat4\",\"job_JobCat5\",\"job_JobCat6\",\"job_JobCat7\",\"job_JobCat8\",\"job_JobCat9\",\"job_JobCat10\",\"marital_divorced\",\"marital_married\",\"marital_single\",\"education_primary\",\"education_secondary\",\"education_tertiary\",\"default_encoded\",\"scaled_balance\",\"housing_encoded\",\"loan_encoded\",\"contact_cellular\",\"contact_telephone\",\"contact_unknown\",\"date\",\"scaled_campaign\",\"scaled_pdays\",\"scaled_previous\",\"poutcome_failure\",\"poutcome_other\",\"poutcome_success\",\"poutcome_unknown\",\"y_encoded\"]\n",
    "final_bank = final_bank[final_order]\n",
    "final_bank.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "In this section, I will use the final_bank dataframe created during preprocessing to train and test various machine learning models using the sci-kit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect input features and target feature\n",
    "features_cols = final_bank.columns[:-1]\n",
    "target_col = final_bank.columns[-1]\n",
    "X = final_bank[features_cols]\n",
    "y = final_bank[target_col]\n",
    "\n",
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am selecting features based on their variance first, to get rid of those with low variance, meaning those where most of the values are the same. I have set the threshold at 0.2, meaning if 80%+ of the values of a feature are the same, that feature will be removed. We have 32 features currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape before variance thresholding\n",
    "print(\"Shape of X_train before variance thresholding:\", X_train.shape)\n",
    "print(\"Shape of X_test before variance thresholding:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variance selector\n",
    "var_selector = VarianceThreshold(threshold=0.2)\n",
    "\n",
    "# fit to training data\n",
    "var_selector.fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train = var_selector.transform(X_train)\n",
    "X_test = var_selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape after variance thresholding\n",
    "print(\"Shape of X_train after variance thresholding:\", X_train.shape)\n",
    "print(\"Shape of X_test after variance thresholding:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "Random Forest models are known to be robust supervised models suitable for classification tasks with in-built feature importance & selection based on node impurity. They are also resilient to noise and modulable. Parameters such as random state, n_estimators and maximum depth will remain the same throughout different iterations and experimentation of the Random forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# fit basic rf model\n",
    "rf0 = RandomForestClassifier(max_depth=15, random_state=2, n_estimators=200)\n",
    "rf0.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred0 = rf0.predict(X_test)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy0 = accuracy_score(y_test, y_pred0)\n",
    "print(\"Accuracy:\", accuracy0)\n",
    "f1_rf0 = f1_score(y_test, y_pred0)\n",
    "print(\"F1_score:\", f1_rf0)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial results show model accuracy is decently high - 88.33%, but has a very low F1-score of 25.70%. This low score in addition to the classification report shows that the model performs well to classify one class (False) over another (True) because of the presence of class imbalance - 4563 vs 637. Before addressing the class imbalance, let's do some more feature selection by applying Recursive Feature Elimination with cross-validation to the training dataset to see if model performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RFECV object\n",
    "selector_rfe = RFECV(estimator=rf0, step=1, cv=5)\n",
    "\n",
    "# fit the RFECV object to training data\n",
    "selector_rfe.fit(X_train, y_train)\n",
    "\n",
    "# get selected features\n",
    "X_train_rfe = selector_rfe.transform(X_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test)\n",
    "\n",
    "# fit RF model\n",
    "rf_rfe = RandomForestClassifier(max_depth=15, random_state=2, n_estimators=200)\n",
    "rf_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_rfe = rf_rfe.predict(X_test_rfe)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "print(\"Accuracy:\", accuracy_rfe)\n",
    "f1_rf_rfe = f1_score(y_test, y_pred_rfe)\n",
    "print(\"F1_score:\", f1_rf_rfe)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rfe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance improved, specifically F1-score increased by 6.49% while accuracy increased by 0.25%. Let's apply a different method: the built-in feature importance score of the Random Forest classifier to select the top 75th percentile features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve feature importance scores\n",
    "feature_importances = rf0.feature_importances_\n",
    "\n",
    "# select top 75th percentile features based on importance scores\n",
    "percentile_value = 75\n",
    "threshold_fi = np.percentile(rf0.feature_importances_, percentile_value)\n",
    "selector_fi = SelectFromModel(rf0, threshold=threshold_fi)\n",
    "selector_fi.fit(X_train, y_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_fi = selector_fi.transform(X_train)\n",
    "X_test_fi = selector_fi.transform(X_test)\n",
    "\n",
    "# train model on selected features\n",
    "rf_fi = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=2)\n",
    "rf_fi.fit(X_train_fi, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_fi = rf_fi.predict(X_test_fi)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy_fi = accuracy_score(y_test, y_pred_fi)\n",
    "print(\"Accuracy:\", accuracy_fi)\n",
    "f1_rf_fi = f1_score(y_test, y_pred_fi)\n",
    "print(\"F1_score:\", f1_rf_fi)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance is similar to the one with RFE cross validation, with slightly lower accuracy and F1-score overall. I will keep the training data with the RFE selected features for the next iterations of the Random Forest classifier model to address class imbalance. The first experiment is to add 'balanced' class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define & fit RF model with balanced weights\n",
    "rf2 = RandomForestClassifier(max_depth=10, random_state=2, class_weight=\"balanced\", n_estimators=200)\n",
    "rf2.fit(X_train_rfe, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred2 = rf2.predict(X_test_rfe)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "f1_rf2 = f1_score(y_test, y_pred2)\n",
    "print(\"F1_score:\", f1_rf2)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the parameter 'balanced' weights to the model increased overall F1-score by 14.97% compared to the RFE Model. Recall & F1 metrics for the 'True' class also increased. Let's experiment with custom weights now: I created a parameter grid to grid search for the best weight values using StratifiedKfold validation, the random forest classifier defined above and f1-scoring.\n",
    "I plotted the scoring of the weights as well to see the evolution of the grid search and to see where the best parameter falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set range for class weights\n",
    "weights = np.linspace(0.0,6.0,25)\n",
    "\n",
    "# create dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0: x, 1: 6.0 - x} for x in weights]}\n",
    "\n",
    "# define grid search with RF classifier, stratified k-fold & f1 scoring\n",
    "gridsearch = GridSearchCV(estimator= rf_rfe, param_grid= param_grid, cv=StratifiedKFold(), n_jobs=2, scoring='f1',verbose=2)\n",
    "\n",
    "# fit the grid search to training data\n",
    "gridsearch.fit(X_train_rfe, y_train)\n",
    "\n",
    "# get the best parameters for both classes\n",
    "best_params = gridsearch.best_params_\n",
    "best_weight_class_0 = best_params['class_weight'][0]\n",
    "best_weight_class_1 = best_params['class_weight'][1]\n",
    "\n",
    "print(\"Best Weight for Class 0 (False):\", best_weight_class_0)\n",
    "print(\"Best Weight for Class 1 (True):\", best_weight_class_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scoring for the two classes\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# plot for class 0 (False - majority)\n",
    "weigh_data0 = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1 - weights)})\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(weigh_data0['weight'], weigh_data0['score'], marker='o')\n",
    "plt.xlabel('Weight for class 0')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i / 2, 1) for i in range(-8, 4, 1)])\n",
    "plt.title('Scoring for different class weights (False Label)', fontsize=16)\n",
    "plt.axvline(x=best_weight_class_0, color='r', linestyle='--', label=f'Best Weight: {best_weight_class_0}')\n",
    "plt.legend()\n",
    "\n",
    "# plot for class 1 (True - minority)\n",
    "weigh_data1 = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (weights)})\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(weigh_data1['weight'], weigh_data1['score'], marker='o')\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i / 2, 1) for i in range(0, 15, 1)])\n",
    "plt.title('Scoring for different class weights (True Label)', fontsize=16)\n",
    "plt.axvline(x=best_weight_class_1, color='r', linestyle='--', label=f'Best Weight: {best_weight_class_1}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best weight parameters are 0.50 and 5.5 respectively for majority Class 0 (False) and minority Class 1 (True). I implement these weights below to see their effect on the performance of the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class weights\n",
    "class_weights = {0: best_weight_class_0, 1: best_weight_class_1}\n",
    "\n",
    "# define & fit RF model with weights\n",
    "rf3 = RandomForestClassifier(max_depth=10, random_state=2, class_weight=class_weights, n_estimators=200)\n",
    "rf3.fit(X_train_rfe, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred3 = rf3.predict(X_test_rfe)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy3 = accuracy_score(y_test, y_pred3)\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "f1_rf3 = f1_score(y_test, y_pred3)\n",
    "print(\"F1_score:\", f1_rf3)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Accuracy and F1-score worsened, dropping by 8.12% and 6.30% respectively compared to 'balanced' weights parameter in the previous model which is interesting. The reason could be that sci-kit's learn 'balanced' parameter automatically adjusts class weights inversely proportional to class frequencies in the training data, allowing it to effectively handle class imbalance compared to grid search which can be sensitive to complex and/or irregular class distribution and depends on the variability of the dataset.<br><br>\n",
    "Moving on, another method to handle class imbalance is to undersample the majority class in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample training data\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_rfe, y_train)\n",
    "\n",
    "# fit RF\n",
    "rf4 = RandomForestClassifier(max_depth=10, random_state=2, n_estimators=200)\n",
    "rf4.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# predict\n",
    "y_pred4 = rf4.predict(X_test_rfe)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy4 = accuracy_score(y_test, y_pred4)\n",
    "print(\"Accuracy:\", accuracy4)\n",
    "f1_rf4 = f1_score(y_test, y_pred4)\n",
    "print(\"F1_score:\", f1_rf4)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling reduced overall model accuracy by 11.27% but increased model F1-score by 9.74% compared to the base RFE model. Recall & F1-score improved significantly for the 'True' class; all three metrics worsened for the 'False' class, similarly to the model where balanced class weights was applied. This is a direct cause of undersampling, which reduces the number of True positives for the majority class. Let's combine the balanced class weights with undersampling to help the model capture more complexities in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit RF classifier with balanced class weights\n",
    "rf5 = RandomForestClassifier(max_depth=10, random_state=2, n_estimators=200, class_weight='balanced')\n",
    "rf5.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# predict\n",
    "y_pred5 = rf5.predict(X_test_rfe)\n",
    "\n",
    "# evaluate metrics\n",
    "accuracy5 = accuracy_score(y_test, y_pred5)\n",
    "print(\"Accuracy:\", accuracy5)\n",
    "f1_rf5 = f1_score(y_test, y_pred5)\n",
    "print(\"F1_score:\", f1_rf5)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding balanced class weights to undersampling did not result in any changes in model performance compared to undersampling on its own. This is probably due to undersampling already handling the class imbalance, giving no 'work' to do for the class weights.<br><br> Regarding metrics, when class imbalance is prevalent, it is beneficial to also calculate ROC and area under ROC curve as they are less sensitive to imbalance compared to acuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabilities of 'True' class for all our model iterations\n",
    "rf_probs0 = rf0.predict_proba(X_test)[:,1]  # basic model pre-RFE\n",
    "rf_probs_rfe = rf_rfe.predict_proba(X_test_rfe)[:,1]  # basic model with RFE\n",
    "rf2_probs2 = rf2.predict_proba(X_test_rfe)[:,1] # balanced class weight model\n",
    "rf3_probs3 = rf3.predict_proba(X_test_rfe)[:,1] # custom class weight model\n",
    "rf4_probs4 = rf4.predict_proba(X_train_resampled)[:,1] # undersampling model\n",
    "rf5_probs5 = rf5.predict_proba(X_train_resampled)[:,1] # undersampling + class weight model\n",
    "\n",
    "# calculate ROC curve\n",
    "rf_fpr0, rf_tpr0, thresholds = roc_curve(y_test, rf_probs0)\n",
    "rf_fpr_rfe, rf_tpr_rfe, thresholds = roc_curve(y_test, rf_probs_rfe)\n",
    "rf_fpr2, rf_tpr2, thresholds = roc_curve(y_test, rf2_probs2)\n",
    "rf_fpr3, rf_tpr3, thresholds = roc_curve(y_test, rf3_probs3)\n",
    "rf_fpr4, rf_tpr4, thresholds = roc_curve(y_train_resampled, rf4_probs4)\n",
    "rf_fpr5, rf_tpr5, thresholds = roc_curve(y_train_resampled, rf5_probs5)\n",
    "\n",
    "# calculate AUC\n",
    "rf_auc0 = roc_auc_score(y_test, rf_probs0)\n",
    "rf_auc_rfe = roc_auc_score(y_test, rf_probs_rfe)\n",
    "rf_auc2 = roc_auc_score(y_test, rf2_probs2)\n",
    "rf_auc3 = roc_auc_score(y_test, rf3_probs3)\n",
    "rf_auc4 = roc_auc_score(y_train_resampled, rf4_probs4)\n",
    "rf_auc5 = roc_auc_score(y_train_resampled, rf5_probs5)\n",
    "\n",
    "\n",
    "# plot ROC curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rf_fpr0, rf_tpr0, color='blue', lw=1, label=f'Basic RF (AUC = {rf_auc0:.2f})')\n",
    "plt.plot(rf_fpr_rfe, rf_tpr_rfe, color='black', lw=1, label=f'Basic RF + RFE (AUC = {rf_auc_rfe:.2f})')\n",
    "plt.plot(rf_fpr2, rf_tpr2, color='red', lw=1, label=f'RF + balanced class weights (AUC = {rf_auc2:.2f})')\n",
    "plt.plot(rf_fpr3, rf_tpr3, color='orange', lw=1, label=f'RF + custom class weights (AUC = {rf_auc3:.2f})')\n",
    "plt.plot(rf_fpr4, rf_tpr4, color='green', lw=1, label=f'RF + undersampling (AUC = {rf_auc4:.2f})')\n",
    "plt.plot(rf_fpr5, rf_tpr5, color='purple', lw=1, label=f'RF + undersampling & custom class weights (AUC = {rf_auc5:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves of 5 RF classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the AUC score, the better the model is at distinguishing between the positive and negative classes. We can see that undersampling significantly improved the model's ability to distinguish from the classes compared to class weights on their own. The basic Random Forest classifier scored similarly to the one with RFE applied. Below I compiled the performance scores of all 6 Random Forest classifiers defined above. As a reminder, all models after the Basic RF + RFE use the training data where Recursive Feature Elimination was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile RF model scores into a dataframe\n",
    "rf_data = [[\"Basic RF\", accuracy0, f1_rf0, rf_auc0],\n",
    "           [\"Basic RF + RFE\", accuracy_rfe, f1_rf_rfe, rf_auc_rfe],\n",
    "           [\"RF + Balanced weights\", accuracy2, f1_rf2, rf_auc2],\n",
    "           [\"RF + custom weights\", accuracy3, f1_rf3, rf_auc3],\n",
    "           [\"RF + undersampling\", accuracy4, f1_rf4, rf_auc4],\n",
    "           [\"RF + undersampling & balanced weights\", accuracy5, f1_rf5, rf_auc5]]\n",
    "rf_scores = pd.DataFrame(rf_data, columns=[\"Model\", \"Accuracy\",\" F1-score\", \"AUC score\"])\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic RF + RFE classifier has the highest accuracy of them all, however the low F1-score suggests the model struggles to capture the minority 'True' class appropriately. Adding balanced weights slightly reduced model accuracy but increased it's ability to better capture the minority class, as indicated by the significantly improved F1-score. The model with the custom weight scored much worse to the ones with balanced weights, indicating that our cross validation was unable to find similar weights to sci-kit's learn parameter.<br>\n",
    "Undersampling severely reduced the model's accuracy, and achieved a lower F1-score compared to the model's with balanced weights; however it achieved the highest AUC score. Adding weights to undersampling did not improve nor worsen model performance.<br><br>\n",
    "Due to the class imbalance present in our training dataset, AUC and F1-score are more important than accuracy. Therefore, our Random Forest Classifier where Recursive Feature Elimination was applied & paired balanced class weights for the target classes is the best performing one with a good balance between accuracy, F1-score and AUC score at 84.79%, 47.16% and 79.34% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save best performing model\n",
    "joblib.dump(rf2, \"rf_model_rfe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "Adaptive Boost is an ensemble learning method that combines multiple machine learning models to create a robust & strong classifier. I will fit the AdaBoost classifier below with some basic classifiers and compare their performance to the Random Forest classifier above. The training data used will be the one post RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define basic models to use in the adaboost for comparison with RF model\n",
    "dt_clf = DecisionTreeClassifier(class_weight='balanced')\n",
    "rf_clf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# define adabost classifier\n",
    "adaboost_clf = AdaBoostClassifier(estimator=None, n_estimators=50, random_state=2, learning_rate=1, algorithm='SAMME')\n",
    "adaboost_clf_dt = AdaBoostClassifier(estimator=dt_clf, n_estimators=50, random_state=2, learning_rate=1, algorithm='SAMME')\n",
    "adaboost_clf_rf = AdaBoostClassifier(estimator=rf_clf, n_estimators=50, random_state=2, learning_rate=1, algorithm='SAMME')\n",
    "\n",
    "# train adaboost classifier\n",
    "adaboost_clf.fit(X_train_rfe, y_train)\n",
    "adaboost_clf_dt.fit(X_train_rfe, y_train)\n",
    "adaboost_clf_rf.fit(X_train_rfe, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_ada = adaboost_clf.predict(X_test_rfe)\n",
    "y_pred_dt = adaboost_clf_dt.predict(X_test_rfe)\n",
    "y_pred_rf = adaboost_clf_rf.predict(X_test_rfe)\n",
    "\n",
    "# calculate probabilities\n",
    "ada_probs = adaboost_clf.predict_proba(X_test_rfe)[:,1]\n",
    "ada_dt_probs = adaboost_clf_dt.predict_proba(X_test_rfe)[:,1]\n",
    "ada_rf_probs = adaboost_clf_rf.predict_proba(X_test_rfe)[:,1]\n",
    "\n",
    "# evaluate adaboost\n",
    "print(\"AdaBoost:\")\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print(\"Accuracy:\", accuracy_ada)\n",
    "f1_ada = f1_score(y_test, y_pred_ada)\n",
    "print(\"F1_score:\", f1_ada)\n",
    "ada_auc = roc_auc_score(y_test, ada_probs)\n",
    "print(\"AUC score:\", ada_auc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada))\n",
    "\n",
    "# evaluate adaboost + dt\n",
    "print(\"\\nAdaBoost + Decision tree:\")\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "f1_ada_dt = f1_score(y_test, y_pred_dt)\n",
    "print(\"F1_score:\", f1_ada_dt)\n",
    "ada_dt_auc = roc_auc_score(y_test, ada_dt_probs)\n",
    "print(\"AUC score:\", ada_dt_auc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# evaluate adaboost + rf\n",
    "print(\"\\nAdaBoost + Random Forest:\")\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "f1_ada_rf = f1_score(y_test, y_pred_rf)\n",
    "print(\"F1_score:\", f1_ada_rf)\n",
    "ada_rf_auc = roc_auc_score(y_test, ada_rf_probs)\n",
    "print(\"AUC score:\", ada_rf_auc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with no estimator scored the highest AUC but the lowest F1-score, indicating it struggled to accuracy predict positive instances of the minority class. AdaBoost with the decision tree estimator scored significantly higher F1 but lower AUC. Lastly, AdaBoost with random forest estimator had the highest accuracy, but scored lower F1 & AUC compared to AdaBoost with decision tree estimator. Looking at the classification report, AdaBoost + RF favoured the majority class with higher recall & F1 compared to AdaBoost + DT. Overall, our previous Random Forest model with RFE & balanced weights performed better than any of our AdaBoost models here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions\n",
    "In this next section, I will use the top performing Random Forest model with balanced weights & recursive feature elimination to predict the class labels on the queries.txt dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queries data for prediction\n",
    "queries_data = pd.read_csv(\"data/queries.txt\", delimiter='\\s*,\\s*', engine=\"python\")\n",
    "queries_data.columns = queries_data.columns.str.strip()\n",
    "queries_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double quotes from values in all object columns\n",
    "for column in queries_data.columns:\n",
    "    if queries_data[column].dtype == object:\n",
    "        queries_data[column] = queries_data[column].str.strip('\"')\n",
    "\n",
    "queries_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply same preprocessing steps as for trainingset.txt\n",
    "# binary encode default, housing and loan\n",
    "queries_bencoded = pd.get_dummies(queries_data, columns=['default', 'housing', 'loan'], drop_first=True)\n",
    "queries_bencoded.columns = queries_bencoded.columns.str.replace('yes', 'encoded')\n",
    "\n",
    "# one-hot encode remaining categorical columns\n",
    "queries_ohencoded = pd.get_dummies(queries_data, columns=['job', 'marital', 'education', 'contact', 'poutcome'])\n",
    "\n",
    "# combine one-hot encoded & binary encoded dataframes\n",
    "queries_encoded = pd.concat([queries_bencoded, queries_ohencoded], axis=1)\n",
    "\n",
    "# drop duplicate cols\n",
    "queries_encoded = queries_encoded.loc[:,~queries_encoded.columns.duplicated()]\n",
    "\n",
    "# drop non-encoded cols\n",
    "queries_encoded.drop([\"job\", \"marital\",\"education\",\"contact\",\"poutcome\",\"default\", \"housing\", \"loan\"], axis=1, inplace=True)\n",
    "\n",
    "# preprocess day & month into one column\n",
    "queries_encoded['date'] = queries_encoded['day'].astype(str) + '-' + queries_encoded['month']\n",
    "queries_encoded['date'] = pd.to_datetime(queries_encoded['date'], format='%d-%b')\n",
    "queries_encoded['date'] = queries_encoded['date'] + pd.offsets.DateOffset(years=2023 - queries_encoded['date'].dt.year.max())\n",
    "queries_encoded.drop(['day', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# drop 'duration' column\n",
    "queries_encoded.drop([\"duration\"], axis=1, inplace=True)\n",
    "\n",
    "# normalize age, balance, campaign, pdays & previous\n",
    "queries_scaled = scaler.fit_transform(queries_encoded[cols_scale])\n",
    "queries_scaled = pd.DataFrame(queries_scaled, columns=cols_scale)\n",
    "queries_scaled = queries_scaled.add_prefix('scaled_')\n",
    "\n",
    "# combine scaled & encoded dataframes\n",
    "queries_encoded.reset_index(drop=True, inplace=True)\n",
    "final_queries = pd.concat([queries_encoded, queries_scaled], axis=1)\n",
    "\n",
    "# drop non-encoded cols\n",
    "final_queries.drop([\"age\", \"previous\",\"campaign\",\"balance\",\"pdays\"], axis=1, inplace=True)\n",
    "\n",
    "# convert date column to int64 for model prediction\n",
    "final_queries['date'] = final_queries['date'].astype(\"int64\")\n",
    "\n",
    "# drop target column\n",
    "final_queries.drop([\"y\"], axis=1, inplace=True)\n",
    "\n",
    "# check new dataframe\n",
    "final_queries.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape before variance selector\n",
    "final_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit variance selector to training data\n",
    "var_selector.fit(final_queries)\n",
    "final_transformed = var_selector.transform(final_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape after variance selector\n",
    "final_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve features selected by RFE\n",
    "selected_feats_idx = selector_rfe.get_support(indices=True)\n",
    "og_col_names = final_queries.columns\n",
    "selected_feats = [og_col_names[i] for i in selected_feats_idx]\n",
    "\n",
    "print(\"Features selected by RFE: \", selected_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict class labels for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "rf_model = joblib.load(\"rf_model_rfe.pkl\")\n",
    "\n",
    "# select queries columns that match RFE columns\n",
    "selected_final_queries = final_transformed[:, selected_feats_idx]\n",
    "\n",
    "# make predictions\n",
    "final_predictions = rf_model.predict(selected_final_queries)\n",
    "\n",
    "# view predictions\n",
    "print(\"Predictions:\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions in a dataframe using loop\n",
    "num_preds = len(final_predictions)\n",
    "predictions_df = pd.DataFrame(columns=['query', 'prediction'])\n",
    "for i in range(num_preds):\n",
    "    predictions_df.loc[i] = [i + 1, final_predictions[i]]\n",
    "\n",
    "# rename True and False values with original class labels\n",
    "predictions_df['prediction'] = predictions_df['prediction'].replace({True: 'TypeB', False: 'TypeA'})\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe as txt file\n",
    "predictions_df.to_csv('data/D22127697.txt', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
